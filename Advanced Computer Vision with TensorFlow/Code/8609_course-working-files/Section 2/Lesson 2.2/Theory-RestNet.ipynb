{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep Residual Network Architectural Design\n",
    "*by Marvin Bertin*\n",
    "<img src=\"../../images/keras-tensorflow-logo.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Residual Networks (ResNets)\n",
    "\n",
    "- Award winning\tsimple\tand\tclean\tarchitecture for\ttraining\t“very”\tdeep\tnets for **computer vision**\n",
    "- Invented by **Microsoft Research** in 2015\n",
    "- Won **1st places**\tin\tall\tfive main tracks in ImageNet and COCO Large Scale Visual Recognition Competition in 2015\n",
    "    - **ImageNet\tClassification**:\t“Ultra-deep”\t152-layer nets\t\n",
    "    - **ImageNet\tDetection**: 16% better\tthan\t2nd\n",
    "    - **ImageNet\tLocalization**: 27% better\tthan\t2nd\n",
    "    - **COCO\tDetection**: 11% better\tthan\t2nd\n",
    "    - **COCO\tSegmentation**: 12% better\tthan\t2nd\n",
    "\n",
    "\n",
    "### Motivation\n",
    "- Network depth is of crucial importance in neural network architectures\n",
    "- Deeper networks are more difficult to train.\n",
    "- Residual learning eases the training\n",
    "- Enables them to be substantially deeper  with improved performance\n",
    "\n",
    "<img src=\"../../images/resnet-results.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../images/resnet-coco.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Increasingly Deeper Networks\n",
    "\n",
    "### Common Practices\n",
    "\n",
    "**Initialization**\n",
    "- Careful initialization of model weights\n",
    "    - Avoid exploding or vanishing gradients during training\n",
    "\n",
    "**Batch Normalization**\n",
    "- Batch Normalization of each layer for each training mini-batch\n",
    "    - Accelerates training\n",
    "    - Less sensitive to initialization, for more stable training\n",
    "    - Improves regularization of model (better generalization)\n",
    "\n",
    "<img src=\"../../images/bn.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simply stacking\tmore layers does not improve performance\n",
    "\n",
    "- 56-layer\tCNN\thas\thigher\t**training error and\ttest\terror**\tthan\t20-layer\tCNN\n",
    "- accuracy gets saturated and then starts degrading\n",
    "\n",
    "<img src=\"../../images/stacking.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet Introduces Residual Learning\n",
    "Original Paper:\n",
    "[Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "\n",
    "** Plain Layer Stacking**\n",
    "\n",
    "<img src=\"../../images/plain-stack.png\" width=\"300\">\n",
    "\n",
    "** Stacking with Residual Connection**\n",
    "\n",
    "- add an identity skip connection\n",
    "- layer learns residual mapping instead\n",
    "- makes optimization easier expecially for deeper layers\n",
    "- helps propagate signal deep into the network with less degradation\n",
    "\n",
    "**New assumption:**\n",
    "- optimal function is closer to an identity mapping than to a zero mapping\n",
    "- easier for network to learn residual error\n",
    "- each layer is responsible for fine-tuning the output of a previous block (instead of having to generate the desired output from scratch)\n",
    "\n",
    "<img src=\"../../images/residual-stack.png\" width=\"400\">\n",
    "\n",
    "\n",
    "# Bottleneck Design\n",
    "\n",
    "** Practical design for going deeper**\n",
    "- sandwich 3x3 conv layer with two 1x1 conv layers\n",
    "- similar complexity\n",
    "- better representation\n",
    "- 1x1 conv reduce tensor dimensonality for 3x3 conv layer\n",
    "\n",
    "<img src=\"../../images/bottleneck.png\" width=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet Provides Improvements in 3 Key Concepts\n",
    "\n",
    "**Representation**\n",
    "- training of much deeper networks\n",
    "- larger feature space allows for better representation\n",
    "\n",
    "**Optimization**\n",
    "- Enable very smooth forward propagation of signal and backward propagation of error\n",
    "- Greatly\tease\toptimizing\tdeeper models\n",
    "\n",
    "**Generalization**\n",
    "- Does not overfit on training data\n",
    "- Maintains good performance on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet Improvement: Pre-Activation\n",
    "\n",
    "Improvements on ResNet design: [Identity Mappings in Deep Residual Networks](https://arxiv.org/pdf/1603.05027.pdf)\n",
    "\n",
    "<img src=\"../../images/pre-activation.png\" width=\"400\">\n",
    "\n",
    "- ReLU\tcould\tblock\tprop\twhen\tthere\tare\t1000\tlayers\n",
    "- pre-activation\tdesign\teases\toptimization\tand\timproves\tgeneralization\n",
    "\n",
    "\n",
    "<img src=\"../../images/pre-activation-results.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Lesson\n",
    "### Implementation of a Deep Residual Neural Network\n",
    "- residual skip connections\n",
    "- bottleneck blocks\n",
    "\n",
    "<img src=\"../../images/divider.png\" width=\"100\">"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:TF-1_3]",
   "language": "python",
   "name": "conda-env-TF-1_3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
